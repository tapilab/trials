{
 "metadata": {
  "name": "",
  "signature": "sha256:d9879c7fa769341062a6cada6141634f3f78afde357ab4733e7ca09c26767723"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Clinical Trial Data\n",
      "\n",
      "We download clinical trials from [ClinicalTrials.gov](http://clinicaltrials.gov) and create a search engine using [Whoosh](https://pythonhosted.org/Whoosh/)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# RAWDIR stores the search results from ClinicalTrials.gov, one trial per xml file.\n",
      "RAWDIR='/data/trials/search_result'\n",
      "\n",
      "# The search index will be stored here.\n",
      "INDEXDIR='/data/trials/index'\n",
      "!mkdir -p $INDEXDIR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import glob \n",
      "import io\n",
      "import sys, traceback\n",
      "\n",
      "from lxml import etree\n",
      "from whoosh.fields import KEYWORD, DATETIME, ID, Schema, TEXT\n",
      "from whoosh.index import create_in\n",
      "from whoosh.qparser import QueryParser\n",
      "\n",
      "class TrialSearcher:\n",
      "    \"\"\" Index a list of trial xml documents to support fielded queries. \"\"\"\n",
      "    \n",
      "    def __init__(self, xml_dir, limit=1000):\n",
      "        \"\"\"\n",
      "        Params:\n",
      "          xml_dir ... path containing a list of xml files, one per trial.\n",
      "          limit ..... maximum number of xml files to index, for testing. \"\"\"\n",
      "        self._create_index(xml_dir, limit)\n",
      "        \n",
      "    def _find_text(self, element, xpathq):\n",
      "        \"\"\" Return the text contents of the results of an xpath query on this element.\n",
      "        Params:\n",
      "          element ... root XML element to search\n",
      "          xpathq .... An XPath query to run.\n",
      "        Returns:\n",
      "          A string containing the text portion of the matching element. If there\n",
      "          multiple matches, they are concatenated. \"\"\"\n",
      "        matches = [x for x in element.xpath(xpathq)]\n",
      "        return u' '.join(unicode(m.text.strip()) for m in matches)\n",
      "    \n",
      "    def _add_doc(self, xmlfile, writer):\n",
      "        \"\"\" Add a single trial XML file as a document in the index.\n",
      "        Params:\n",
      "          xmlfile ... Path to trial XML file.\n",
      "          writer .... The IndexWriter.\"\"\"\n",
      "        tree = etree.parse(io.open(xmlfile, encoding='utf8'))\n",
      "        for study in tree.xpath(\"//clinical_study\"):\n",
      "            d = {\n",
      "                'completion_date': self._find_text(study, '//completion_date'),\n",
      "                'criteria': self._find_text(study, '//eligibility/criteria/textblock'),\n",
      "                'gender': self._find_text(study, '//eligibility/gender'),\n",
      "                'healthy_volunteers': self._find_text(study, '//eligibility/healthy_volunteers'),\n",
      "                'locations': self._find_text(study, '//location_countries/country'),\n",
      "                'maximum_age': self._find_text(study, '//eligibility/maximum_age'),\n",
      "                'minimum_age': self._find_text(study, '//eligibility/minimum_age'),\n",
      "                'nct_id': self._find_text(study, '//id_info/nct_id'),\n",
      "                'title': self._find_text(study, '//brief_title'),\n",
      "                'source': self._find_text(study, '//source'),\n",
      "                'start_date': self._find_text(study, '//start_date'),\n",
      "                }\n",
      "            # FIXME: consider Boolean, Numeric field types.\n",
      "            writer.add_document(**d)\n",
      "        \n",
      "    def _create_index(self, xml_dir, limit):\n",
      "        \"\"\" Create an index containing the contents of the trial xml directory.\n",
      "        Params:\n",
      "          xml_dir ... path to list of trial xml files.\n",
      "          limit ..... Only read this many xml files, for testing.\"\"\"\n",
      "        schema = Schema(\n",
      "                        completion_date=TEXT,\n",
      "                        criteria=TEXT(stored=True),\n",
      "                        gender=TEXT,\n",
      "                        healthy_volunteers=TEXT,\n",
      "                        locations=TEXT,\n",
      "                        maximum_age=ID,\n",
      "                        minimum_age=ID,\n",
      "                        nct_id=ID(stored=True),\n",
      "                        title=TEXT(stored=True),\n",
      "                        source=TEXT,\n",
      "                        start_date=TEXT\n",
      "                        )\n",
      "\n",
      "        self.index = create_in(INDEXDIR, schema)\n",
      "        writer = self.index.writer(limitmb=1000)\n",
      "        count = 0\n",
      "        for xmlfile in glob.glob(xml_dir + '/*.xml'):\n",
      "            try:\n",
      "                self._add_doc(xmlfile, writer)\n",
      "            except:\n",
      "                print 'cannot parse file %s' % xmlfile\n",
      "            count += 1\n",
      "            if count % 100 == 0:\n",
      "                print 'indexed %d documents' % count\n",
      "            if count == limit:\n",
      "                break\n",
      "        writer.commit()\n",
      "\n",
      "    def search(self, query_str, limit=100):\n",
      "        \"\"\" Exectue a query on this index.\n",
      "        Params:\n",
      "          query_str: A possibly fielded query string. Searches the 'criteria' field by default.\n",
      "          limit: Maximum number of results to return.\n",
      "        Return:\n",
      "          A rank-ordered list of document ids.\"\"\"\n",
      "        parser = QueryParser('criteria', schema=self.index.schema)\n",
      "        with self.index.searcher() as searcher:\n",
      "            query = parser.parse(query_str)\n",
      "            print 'query=', query\n",
      "            results = searcher.search(query, limit=limit)\n",
      "            return [r.docnum for r in results]\n",
      "    \n",
      "    def print_results(self, results):\n",
      "        \"\"\" Print to stdout a list of search results.\n",
      "        Params:\n",
      "          results: A rank-ordered list of document ids.\"\"\"\n",
      "        with self.index.searcher() as searcher:\n",
      "            for r in results:\n",
      "                doc = searcher.stored_fields(r)\n",
      "                print '\\t'.join([doc['nct_id'], doc['title']])\n",
      "\n",
      "searcher = TrialSearcher(RAWDIR, limit=4000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "indexed 100 documents\n",
        "indexed 200 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 300 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 400 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 500 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 600 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 700 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 800 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 900 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1000 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1100 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1200 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1300 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1400 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1500 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1600 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1700 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1800 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 1900 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2000 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2100 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2200 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2300 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2400 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2500 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2600 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2700 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2800 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 2900 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3000 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3100 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3200 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3300 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3400 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3500 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3600 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3700 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3800 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 3900 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexed 4000 documents"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example query.\n",
      "results = searcher.search(u'gender:both AND criteria:cancer AND criteria:thyroid AND healthy_volunteers:Accepts')  # Caution! no spaces after ':'\n",
      "print 'results='\n",
      "searcher.print_results(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "query= (gender:both AND criteria:cancer AND criteria:thyroid AND healthy_volunteers:accepts)\n",
        "results=\n",
        "NCT01149161\tExtent of Central Lymph Node Dissection in Papillary Thyroid Microcarcinoma"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NCT01109420\tClinical and Genetic Studies in Familial Non-medullary Thyroid Cancer\n",
        "NCT00999557\tBimatoprost Ophthalmic Solution in Increasing Eyebrow and Eyelash Growth in Patients Who Have Undergone Chemotherapy for Breast Cancer and in Healthy Participants\n"
       ]
      }
     ],
     "prompt_number": 164
    }
   ],
   "metadata": {}
  }
 ]
}